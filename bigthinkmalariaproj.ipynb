{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # our favorite framework for machine learning!\n",
    "# Note that keras is imported by default, so we'll often call tf.keras.<command>\n",
    "\n",
    "import math\n",
    "import numpy as np # the fundamental building block of ML: arrays!\n",
    "import matplotlib.pyplot as plt # this will help us plot and visualize our data\n",
    "import logging\n",
    "import seaborn as sns # this will help us in understanding performance metrics towards the end\n",
    "\n",
    "import tensorflow_datasets as tfds \n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#INITIALIZING VARIABLES\n",
    "\n",
    "# We will use these variables for visualization of the dataset later.\n",
    "ds, info = tfds.load('malaria', split = 'train', shuffle_files = True, with_info = True)\n",
    "\n",
    "# We will use these variables for actually training the model.\n",
    "train_ds, test_ds = tfds.load(\n",
    "  'malaria',\n",
    "  split = ['train[:70%]', 'train[70%:]'],\n",
    "  shuffle_files = True, as_supervised = True,\n",
    ")\n",
    "\n",
    "#Seeing how many individual images will be both trained and tested \n",
    "NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(NUM_TRAIN_IMAGES)\n",
    "\n",
    "NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "print(NUM_TEST_IMAGES)\n",
    "\n",
    "#Visualalize the dataset with a small sample\n",
    "vis = tfds.visualization.show_examples(ds, info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PROCESSING DATA\n",
    "\n",
    "#The labels in this raw dataset are unintuitive, with 0 representing parasitized and 1 uninfected\n",
    "\n",
    "#The sizes of the images are also 140 x 120 pixels, which can be see with the function below\n",
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image size: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())\n",
    "\n",
    "#To clean the data, we will resize the images to be 200 x 200 pixels \n",
    "# and invert the labels to have 0 represent uninfected cells and 1 represent parasitized ones\n",
    "\n",
    "#These new sets will be represented by clean_train_ds and clean_test_ds\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = [200, 200]\n",
    "\n",
    "def convert(image, label):\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  return image, label\n",
    "\n",
    "# resizing each image to 200 x 200\n",
    "def pad(image,label): \n",
    "  image,label = convert(image, label)\n",
    "  image = tf.image.resize_with_crop_or_pad(image, 200, 200)\n",
    "  return image, label\n",
    "\n",
    "# switching the 0 and 1 around, as mentioned above\n",
    "def invert_labels(image, label):\n",
    " return image, label\n",
    "\n",
    "clean_train_ds = (\n",
    "    train_ds\n",
    "    .map(pad)\n",
    "    .map(invert_labels)\n",
    ")\n",
    "\n",
    "clean_test_ds = (\n",
    "    test_ds\n",
    "    .map(pad)\n",
    "    .map(invert_labels)\n",
    ") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#THE CLEAN DATA\n",
    "\n",
    "#Visualizing the data\n",
    "image_batch, label_batch = next(iter(clean_train_ds.batch(BATCH_SIZE)))\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5, 5, n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        if label_batch[n]:\n",
    "            plt.title(\"parasitized (1) \")\n",
    "        else:\n",
    "            plt.title(\"uninfected (0) \")\n",
    "        plt.axis(\"off\")\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())\n",
    "\n",
    "#Sending the data in batches to the model for training and testing\n",
    "clean_train_ds = clean_train_ds.repeat().shuffle(NUM_TRAIN_IMAGES).batch(BATCH_SIZE)\n",
    "clean_test_ds = clean_test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#THE MODEL\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                             \n",
    "    # CNN: this is the convolutional part of the neural network, how the computer sees the cell \n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu, input_shape = (200, 200, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
    "\n",
    "    # Dense and output layers:\n",
    "  tf.keras.layers.Flatten(), \n",
    "  tf.keras.layers.Dense(300, activation = tf.nn.relu), \n",
    "  tf.keras.layers.Dense(300, activation = tf.nn.relu), \n",
    "  tf.keras.layers.Dense(200, activation = tf.nn.relu), \n",
    "  tf.keras.layers.Dense(200, activation = tf.nn.relu),\n",
    "  tf.keras.layers.Dense(100, activation = tf.nn.relu), \n",
    "  tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
    "  \n",
    "])\n",
    "\n",
    "model.summary() # this is going to print a quick little summary of our model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#COMPILING THE MODEL\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = [tf.keras.metrics.TruePositives(), \n",
    "                         tf.keras.metrics.TrueNegatives(), \n",
    "                         tf.keras.metrics.FalsePositives(), \n",
    "                         tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TRAINING\n",
    "\n",
    "NUMBER_OF_EPOCHS = 5\n",
    "\n",
    "model.fit(clean_train_ds, epochs = NUMBER_OF_EPOCHS, steps_per_epoch = math.ceil(NUM_TRAIN_IMAGES / BATCH_SIZE))\n",
    "\n",
    "#Results from training\n",
    "print(\"\"\"Epoch 1/5\n",
    "603/603 [==============================] - 94s 99ms/step - loss: 0.4499 - true_positives: 7784.0000 - true_negatives: 7205.0000 - false_positives: 2407.0000 - false_negatives: 1900.0000\n",
    "Epoch 2/5\n",
    "603/603 [==============================] - 62s 103ms/step - loss: 0.2367 - true_positives: 9111.0000 - true_negatives: 8731.0000 - false_positives: 912.0000 - false_negatives: 542.0000\n",
    "Epoch 3/5\n",
    "603/603 [==============================] - 61s 102ms/step - loss: 0.1923 - true_positives: 9105.0000 - true_negatives: 9032.0000 - false_positives: 585.0000 - false_negatives: 574.0000\n",
    "Epoch 4/5\n",
    "603/603 [==============================] - 61s 100ms/step - loss: 0.1716 - true_positives: 9220.0000 - true_negatives: 9075.0000 - false_positives: 604.0000 - false_negatives: 397.0000\n",
    "Epoch 5/5\n",
    "603/603 [==============================] - 62s 102ms/step - loss: 0.1569 - true_positives: 9167.0000 - true_negatives: 9132.0000 - false_positives: 596.0000 - false_negatives: 401.0000\n",
    "<keras.callbacks.History at 0x7f0ac2a31050>\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PERFORMANCE ON TEST DATA\n",
    "\n",
    "#Beginning testing\n",
    "test_loss, test_tp, test_tn, test_fp, test_fn = model.evaluate(clean_test_ds, steps = math.ceil(NUM_TEST_IMAGES/BATCH_SIZE))\n",
    "\n",
    "#Creating a confusion matrix using Seaborn \n",
    "def draw_confusion_matrix(tp, tn, fp, fn):\n",
    "  cf_matrix = np.array([[tp, fp], [fn, tn]])\n",
    "  group_names = ['True Pos','False Pos','False Neg','True Neg']\n",
    "  group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "  group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "  labels = np.asarray(labels).reshape(2,2)\n",
    "  sns.heatmap(cf_matrix, annot = labels, fmt = '', cmap = 'Blues', xticklabels = False, yticklabels = False)\n",
    "  \n",
    "  draw_confusion_matrix(test_tp, test_tn, test_fp, test_fn)\n",
    "\n",
    "#Calculating Accuracy\n",
    "accuracy = (test_tp + test_tn) / (test_tp + test_tn + test_fp + test_fn)\n",
    "print(\"The accuracy of this model is %.7f, or about %d%%.\" % (accuracy, round(accuracy*100)))\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6a9681e246f1d35dfd05c6a8a23e6c166d031d8074553cd0cc64adb834264ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
